{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. 텐서플로를 사용한 데이터 적재와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# 깔금한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)    \n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 데이터 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    }
   ],
   "source": [
    "X_nested = {'a': ([1, 2, 3], [4, 5, 6]), 'b': [7, 8, 9]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.1 연쇄 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.2 데이터 셔플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.3 여러 파일에서 한 줄씩 번갈아 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = Path() / \"datasets\" / \"housing\"\n",
    "    housing_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename_format = \"my_{}_{:02d}.csv\"\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    chunks = np.array_split(np.arange(m), n_parts)\n",
    "    for file_idx, row_indices in enumerate(chunks):\n",
    "        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(str(part_csv))\n",
    "        with open(part_csv, \"w\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/housing/my_train_00.csv',\n",
       " 'datasets/housing/my_train_01.csv',\n",
       " 'datasets/housing/my_train_02.csv',\n",
       " 'datasets/housing/my_train_03.csv',\n",
       " 'datasets/housing/my_train_04.csv',\n",
       " 'datasets/housing/my_train_05.csv',\n",
       " 'datasets/housing/my_train_06.csv',\n",
       " 'datasets/housing/my_train_07.csv',\n",
       " 'datasets/housing/my_train_08.csv',\n",
       " 'datasets/housing/my_train_09.csv',\n",
       " 'datasets/housing/my_train_10.csv',\n",
       " 'datasets/housing/my_train_11.csv',\n",
       " 'datasets/housing/my_train_12.csv',\n",
       " 'datasets/housing/my_train_13.csv',\n",
       " 'datasets/housing/my_train_14.csv',\n",
       " 'datasets/housing/my_train_15.csv',\n",
       " 'datasets/housing/my_train_16.csv',\n",
       " 'datasets/housing/my_train_17.csv',\n",
       " 'datasets/housing/my_train_18.csv',\n",
       " 'datasets/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length = n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.226,52.0,5.372469635627531,0.9473684210526315,1157.0,2.3421052631578947,37.96,-121.31,1.076', shape=(), dtype=string)\n",
      "tf.Tensor(b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504', shape=(), dtype=string)\n",
      "tf.Tensor(b'4.7361,7.0,7.464968152866242,1.1178343949044587,846.0,2.694267515923567,34.49,-117.27,1.745', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.4 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean, X_std = scaler.mean_, scaler.scale_\n",
    "n_inputs = 8\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "def preprocess(line):\n",
    "    x, y = parse_csv_line(line)\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543047 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.5 데이터 적재와 전처리 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None, n_parse_threads=5, shuffle_buffer_size = 10_000, seed=42, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length = n_readers, num_parallel_calls=n_read_threads\n",
    "    )\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.6 프리페치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.7 케라스와 데이터셋 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 TFRecord 포맷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
    "    f.write(b'This is the first record')\n",
    "    f.write(b'And this is the second record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 13:51:34.781745: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-11-14 13:51:34.781862: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-11-14 13:51:34.782257: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-11-14 13:51:34.782977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-14 13:51:34.783830: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "filepaths = ['my_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.1 압축된 TFRecord 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_compressed.tfrecord', options) as f:\n",
    "    f.write(b'Compress, compress, compress!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(['my_compressed.tfrecord'], compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.2 프로토콜 버퍼 개요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing person.proto\n"
     ]
    }
   ],
   "source": [
    "%%writefile person.proto\n",
    "syntax = 'proto3';\n",
    "message Person {\n",
    "    string name = 1;\n",
    "    int32 id = 2;\n",
    "    repeated string email = 3;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person.desc    person.proto   person_pb2.py\n"
     ]
    }
   ],
   "source": [
    "%ls person*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Al\"\n",
      "id: 123\n",
      "email: \"a@b.com\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from person_pb2 import Person\n",
    "person = Person(name='Al', id=123, email=['a@b.com'])\n",
    "print(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Al'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.name = 'Alice'\n",
    "person.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a@b.com'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.email[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a@b.com', 'c@d.com']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.email.append('c@d.com')\n",
    "person.email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized = person.SerializeToString()\n",
    "serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person2 = Person()\n",
    "person2.ParseFromString(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person == person2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.3 텐서플로 프로토콜 버퍼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features = Features(\n",
    "        feature={\n",
    "            'name': Feature(bytes_list=BytesList(value=[b'Alice'])),\n",
    "            'id': Feature(int64_list=Int64List(value=[123])),\n",
    "            'emails': Feature(bytes_list=BytesList(value=[b'a@b.com', b'c@d.com']))\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter('my_contacts.tfrecord') as f:\n",
    "    for _ in range(5):\n",
    "        f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.4 Example 프로토콜 버퍼 읽고 파싱하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n"
     ]
    }
   ],
   "source": [
    "feature_description = {\n",
    "    'name': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'id': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'emails': tf.io.VarLenFeature(tf.string)\n",
    "}\n",
    "\n",
    "def parse(serialized_example):\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(['my_contacts.tfrecord']).map(parse)\n",
    "for parsed_example in dataset:\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(parsed_example['emails'], default_value=b'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example['emails'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]], shape=(4, 2), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com' b'a@b.com' b'c@d.com'], shape=(4,), dtype=string), dense_shape=tf.Tensor([2 2], shape=(2,), dtype=int64)), 'id': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([123, 123])>, 'name': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Alice', b'Alice'], dtype=object)>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]], shape=(4, 2), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com' b'a@b.com' b'c@d.com'], shape=(4,), dtype=string), dense_shape=tf.Tensor([2 2], shape=(2,), dtype=int64)), 'id': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([123, 123])>, 'name': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Alice', b'Alice'], dtype=object)>}\n",
      "{'emails': SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([1 2], shape=(2,), dtype=int64)), 'id': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([123])>, 'name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Alice'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "def parse(serialized_example):\n",
    "    return tf.io.parse_example(serialized_example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(['my_contacts.tfrecord']).batch(2).map(parse)\n",
    "for parsed_example in dataset:\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.5 SequenceExample 프로토콜 버퍼로 리스트의 리스트 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import FeatureList, FeatureLists, SequenceExample\n",
    "\n",
    "context = Features(feature = {\n",
    "    'author_id': Feature(int64_list = Int64List(value=[123])),\n",
    "    'title': Feature(bytes_list = BytesList(value = [b'A', b'desert', b'place', b'.'])),\n",
    "    'pub_date': Feature(int64_list = Int64List(value = [1623, 12, 25]))\n",
    "})\n",
    "\n",
    "content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n",
    "           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n",
    "comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n",
    "            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n",
    "\n",
    "def words_to_feature(words):\n",
    "    return Feature(bytes_list = BytesList(value=[word.encode('utf-8') for word in words]))\n",
    "\n",
    "content_features = [words_to_feature(sentence) for sentence in content]\n",
    "comments_features = [words_to_feature(comment) for comment in comments]\n",
    "\n",
    "sequence_example = SequenceExample(\n",
    "    context = context,\n",
    "    feature_lists = FeatureLists(feature_list = {\n",
    "        'content': FeatureList(feature = content_features),\n",
    "        'comments': FeatureList(feature = comments_features)\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context {\n",
       "  feature {\n",
       "    key: \"author_id\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 123\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"pub_date\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1623\n",
       "        value: 12\n",
       "        value: 25\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"title\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"A\"\n",
       "        value: \"desert\"\n",
       "        value: \"place\"\n",
       "        value: \".\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "feature_lists {\n",
       "  feature_list {\n",
       "    key: \"comments\"\n",
       "    value {\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"the\"\n",
       "          value: \"hurlyburly\"\n",
       "          value: \"\\'s\"\n",
       "          value: \"done\"\n",
       "          value: \".\"\n",
       "        }\n",
       "      }\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"the\"\n",
       "          value: \"battle\"\n",
       "          value: \"\\'s\"\n",
       "          value: \"lost\"\n",
       "          value: \"and\"\n",
       "          value: \"won\"\n",
       "          value: \".\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature_list {\n",
       "    key: \"content\"\n",
       "    value {\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"shall\"\n",
       "          value: \"we\"\n",
       "          value: \"three\"\n",
       "          value: \"meet\"\n",
       "          value: \"again\"\n",
       "          value: \"?\"\n",
       "        }\n",
       "      }\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"In\"\n",
       "          value: \"thunder\"\n",
       "          value: \",\"\n",
       "          value: \"lightning\"\n",
       "          value: \",\"\n",
       "          value: \"or\"\n",
       "          value: \"in\"\n",
       "          value: \"rain\"\n",
       "          value: \"?\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"\\nL\\n\\x12\\n\\tauthor_id\\x12\\x05\\x1a\\x03\\n\\x01{\\n \\n\\x05title\\x12\\x17\\n\\x15\\n\\x01A\\n\\x06desert\\n\\x05place\\n\\x01.\\n\\x14\\n\\x08pub_date\\x12\\x08\\x1a\\x06\\n\\x04\\xd7\\x0c\\x0c\\x19\\x12\\xd0\\x01\\nj\\n\\x07content\\x12_\\n*\\n(\\n\\x04When\\n\\x05shall\\n\\x02we\\n\\x05three\\n\\x04meet\\n\\x05again\\n\\x01?\\n1\\n/\\n\\x02In\\n\\x07thunder\\n\\x01,\\n\\tlightning\\n\\x01,\\n\\x02or\\n\\x02in\\n\\x04rain\\n\\x01?\\nb\\n\\x08comments\\x12V\\n&\\n$\\n\\x04When\\n\\x03the\\n\\nhurlyburly\\n\\x02's\\n\\x04done\\n\\x01.\\n,\\n*\\n\\x04When\\n\\x03the\\n\\x06battle\\n\\x02's\\n\\x04lost\\n\\x03and\\n\\x03won\\n\\x01.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_sequence_example = sequence_example.SerializeToString()\n",
    "serialized_sequence_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_feature_descriptions = {\n",
    "    'author_id': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'title': tf.io.VarLenFeature(tf.string),\n",
    "    'pub_date': tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0])\n",
    "}\n",
    "sequence_feature_descriptions = {\n",
    "    \"content\": tf.io.VarLenFeature(tf.string),\n",
    "    \"comments\": tf.io.VarLenFeature(tf.string),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
    "    serialized_sequence_example, context_feature_descriptions, sequence_feature_descriptions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': SparseTensor(indices=tf.Tensor(\n",
       " [[0]\n",
       "  [1]\n",
       "  [2]\n",
       "  [3]], shape=(4, 1), dtype=int64), values=tf.Tensor([b'A' b'desert' b'place' b'.'], shape=(4,), dtype=string), dense_shape=tf.Tensor([4], shape=(1,), dtype=int64)),\n",
       " 'author_id': <tf.Tensor: shape=(), dtype=int64, numpy=123>,\n",
       " 'pub_date': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([1623,   12,   25])>}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'A', b'desert', b'place', b'.'], dtype=object)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_context['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'When', b'shall', b'we', b'three', b'meet', b'again', b'?'],\n",
       " [b'In', b'thunder', b',', b'lightning', b',', b'or', b'in', b'rain', b'?']]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.RaggedTensor.from_sparse(parsed_feature_lists['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 케라스의 전처리 층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.1 Normalization 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 14:00:59.624789: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-11-15 14:00:59.624823: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-11-15 14:00:59.624837: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-11-15 14:00:59.625371: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-15 14:00:59.626298: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "2023-11-15 14:00:59.978533: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  1/363 [..............................] - ETA: 2:44 - loss: 7.9217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 14:01:01.477713: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node SGD/AssignVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 2.6793 - val_loss: 0.8038\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.6527 - val_loss: 1.3866\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5723 - val_loss: 0.6264\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5588 - val_loss: 0.5102\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5521 - val_loss: 0.6031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x299341540>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "model = tf.keras.models.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss = 'mse', optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "norm_layer.adapt(X_train)\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "X_train_scaled = norm_layer(X_train)\n",
    "X_valid_scaled = norm_layer(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 14:03:53.167167: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node SGD/AssignVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 2.8450 - val_loss: 1.1329\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.7795 - val_loss: 0.8731\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6555 - val_loss: 1.1817\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6322 - val_loss: 0.8371\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6109 - val_loss: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a000e7d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[0.9640682],\n",
       "       [1.5896187],\n",
       "       [2.5701478]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = tf.keras.Sequential([norm_layer, model])\n",
    "X_new = X_test[:3]\n",
    "y_pred = final_model(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(5)\n",
    "dataset = dataset.map(lambda X, y: (norm_layer(X), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNormalization(tf.keras.layers.Layer):\n",
    "    def adapt(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
    "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        return(inputs - self.mean_) / (self.std_ + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_norm_layer = MyNormalization()\n",
    "my_norm_layer.adapt(X_train)\n",
    "X_train_scaled = my_norm_layer(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.2 Discretization 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n",
    "discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
    "discretize_layer.adapt(age)\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.3 CategoryEncoding 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
    "onehot_layer(age_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])\n",
    "onehot_layer(two_age_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)\n",
    "onehot_layer(two_age_categories + [0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.4 StringLookup 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = ['Auckland', 'Paris', 'Paris', 'San Francisco']\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([['Paris'], ['Auckland'], ['Auckland'], ['Montreal']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 367 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x2d0801bd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 367 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x2d0801bd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(output_mode='one_hot')\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([['Paris'], ['Auckland'], ['Auckland'], ['Montreal']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[5],\n",
       "       [7],\n",
       "       [4],\n",
       "       [3],\n",
       "       [4]])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([['Paris'], ['Auckland'], ['Foo'], ['Bar'], ['Baz']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.5 Hashing 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [9],\n",
       "       [1]])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_layer = tf.keras.layers.Hashing(num_bins = 10)\n",
    "hashing_layer([['Paris'], ['Tokyo'], ['Auckland'], ['Montreal']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.6 임베딩을 사용해 범부형 특성 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.04719822,  0.04983213],\n",
       "       [ 0.03705468, -0.015729  ],\n",
       "       [-0.04719822,  0.04983213]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
    "embedding_layer(np.array([2, 4, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 0.01484182, -0.00484287],\n",
       "       [ 0.0200063 , -0.02362906],\n",
       "       [ 0.01484182, -0.00484287]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "ocean_prox = ['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND']\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(ocean_prox)\n",
    "lookup_and_embed = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=[], dtype=tf.string),\n",
    "    str_lookup_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocab_size(), output_dim=2)\n",
    "])\n",
    "\n",
    "lookup_and_embed(np.array([\"<1H OCEAN\", \"ISLAND\", \"<1H OCEAN\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = tf.keras.layers.Input(shape=[8], name='num')\n",
    "cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name='cat')\n",
    "cat_embeddings = lookup_and_embed(cat_input)\n",
    "encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\n",
    "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
    "model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.7 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
       "array([[2, 1, 0, 0],\n",
       "       [6, 2, 1, 2]])>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
    "text_vec_layer = tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
       "array([[0.96725637, 0.6931472 , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.96725637, 1.3862944 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 ]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(output_mode='tf_idf')\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.8 사전 훈련된 언어 모델 구성 요소 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25,  0.28,  0.01,  0.1 ,  0.14,  0.16,  0.25,  0.02,  0.07,\n",
       "         0.13, -0.19,  0.06, -0.04, -0.07,  0.  , -0.08, -0.14, -0.16,\n",
       "         0.02, -0.24,  0.16, -0.16, -0.03,  0.03, -0.14,  0.03, -0.09,\n",
       "        -0.04, -0.14, -0.19,  0.07,  0.15,  0.18, -0.23, -0.07, -0.08,\n",
       "         0.01, -0.01,  0.09,  0.14, -0.03,  0.03,  0.08,  0.1 , -0.01,\n",
       "        -0.03, -0.07, -0.1 ,  0.05,  0.31],\n",
       "       [-0.2 ,  0.2 , -0.08,  0.02,  0.19,  0.05,  0.22, -0.09,  0.02,\n",
       "         0.19, -0.02, -0.14, -0.2 , -0.04,  0.01, -0.07, -0.22, -0.1 ,\n",
       "         0.16, -0.44,  0.31, -0.1 ,  0.23,  0.15, -0.05,  0.15, -0.13,\n",
       "        -0.04, -0.08, -0.16, -0.1 ,  0.13,  0.13, -0.18, -0.04,  0.03,\n",
       "        -0.1 , -0.07,  0.07,  0.03, -0.08,  0.02,  0.05,  0.07, -0.14,\n",
       "        -0.1 , -0.18, -0.13, -0.04,  0.15]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_layer = hub.KerasLayer('https://tfhub.dev/google/nnlm-en-dim50/2')\n",
    "sentence_embeddings = hub_layer(tf.constant(['To be', 'Not to be']))\n",
    "sentence_embeddings.numpy().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.9 이미지 전처리 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "images = load_sample_images()['images']\n",
    "crop_image_layer = tf.keras.layers.CenterCrop(height = 100, width = 100)\n",
    "cropped_images = crop_image_layer(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 텐서플로 데이터셋 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_set = train_set.shuffle(len(X_train), seed=42)\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import Example, Feature, Features, BytesList, Int64List\n",
    "\n",
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    return Example(\n",
    "        features = Features(\n",
    "            feature = {\n",
    "                'image': Feature(bytes_list = BytesList(value = [image_data.numpy()])),\n",
    "                'label': Feature(int64_list = Int64List(value = [label]))\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"image\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\rI\\000\\000\\001\\004\\000\\000\\000\\000\\001\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000$\\210\\177>6\\000\\000\\000\\001\\003\\004\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000f\\314\\260\\206\\220{\\027\\000\\000\\000\\000\\014\\n\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\233\\354\\317\\262k\\234\\241m@\\027M\\202H\\017\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000E\\317\\337\\332\\330\\330\\243\\177yz\\222\\215X\\254B\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\001\\001\\000\\310\\350\\350\\351\\345\\337\\337\\327\\325\\244\\177{\\304\\345\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\267\\341\\330\\337\\344\\353\\343\\340\\336\\340\\335\\337\\365\\255\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\301\\344\\332\\325\\306\\264\\324\\322\\323\\325\\337\\334\\363\\312\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\003\\000\\014\\333\\334\\324\\332\\300\\251\\343\\320\\332\\340\\324\\342\\305\\3214\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000c\\364\\336\\334\\332\\313\\306\\335\\327\\325\\336\\334\\365w\\2478\\000\\000\\000\\000\\000\\000\\000\\000\\000\\004\\000\\0007\\354\\344\\346\\344\\360\\350\\325\\332\\337\\352\\331\\331\\321\\\\\\000\\000\\000\\001\\004\\006\\007\\002\\000\\000\\000\\000\\000\\355\\342\\331\\337\\336\\333\\336\\335\\330\\337\\345\\327\\332\\377M\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000>\\221\\314\\344\\317\\325\\335\\332\\320\\323\\332\\340\\337\\333\\327\\340\\364\\237\\000\\000\\000\\000\\000\\022,Rk\\275\\344\\334\\336\\331\\342\\310\\315\\323\\346\\340\\352\\260\\274\\372\\370\\351\\356\\327\\000\\0009\\273\\320\\340\\335\\340\\320\\314\\326\\320\\321\\310\\237\\365\\301\\316\\337\\377\\377\\335\\352\\335\\323\\334\\350\\366\\000\\003\\312\\344\\340\\335\\323\\323\\326\\315\\315\\315\\334\\360P\\226\\377\\345\\335\\274\\232\\277\\322\\314\\321\\336\\344\\341\\000b\\351\\306\\322\\336\\345\\345\\352\\371\\334\\302\\327\\331\\361AIju\\250\\333\\335\\327\\331\\337\\337\\340\\345\\035K\\314\\324\\314\\301\\315\\323\\341\\330\\271\\305\\316\\306\\325\\360\\303\\343\\365\\357\\337\\332\\324\\321\\336\\334\\335\\346C0\\313\\267\\302\\325\\305\\271\\276\\302\\300\\312\\326\\333\\335\\334\\354\\341\\330\\307\\316\\272\\265\\261\\254\\265\\315\\316s\\000z\\333\\301\\263\\253\\267\\304\\314\\322\\325\\317\\323\\322\\310\\304\\302\\277\\303\\277\\306\\300\\260\\234\\247\\261\\322\\\\\\000\\000J\\275\\324\\277\\257\\254\\257\\265\\271\\274\\275\\274\\301\\306\\314\\321\\322\\322\\323\\274\\274\\302\\300\\330\\252\\000\\002\\000\\000\\000B\\310\\336\\355\\357\\362\\366\\363\\364\\335\\334\\301\\277\\263\\266\\266\\265\\260\\246\\250c:\\000\\000\\000\\000\\000\\000\\000\\000\\000(=,H)#\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"label\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 9\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for image, label in valid_set.take(1):\n",
    "    print(create_example(image, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def write_tfrecords(name, dataset, n_shards=10):\n",
    "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
    "             for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "                   for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:35:11.598865: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2023-11-16 21:35:24.464716: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2023-11-16 21:35:25.639523: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    }
   ],
   "source": [
    "train_filepaths = write_tfrecords(\"./datasets/fashion_mnist/my_fashion_mnist.train\", train_set)\n",
    "valid_filepaths = write_tfrecords(\"./datasets/fashion_mnist/my_fashion_mnist.valid\", valid_set)\n",
    "test_filepaths = write_tfrecords(\"./datasets/fashion_mnist/my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
    "                  n_parse_threads=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths,\n",
    "                                      num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(valid_filepaths)\n",
    "test_set = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for X, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(X[i].numpy(), cmap='binary')\n",
    "        plt.axis('off')\n",
    "        plt.title(str(y[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:45:30.994769: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "standardization = tf.keras.layers.Normalization(input_shape=[28, 28])\n",
    "sample_image_batches = train_set.take(100).map(lambda image, lable: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()), axis=0).astype(np.float32)\n",
    "standardization.adapt(sample_images)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    standardization,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:46:50.563539: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2023-11-16 21:46:50.563577: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2023-11-16 21:46:50.565422: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     30/Unknown - 1s 11ms/step - loss: 1.4142 - accuracy: 0.5240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:46:51.799399: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2023-11-16 21:46:51.799412: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2023-11-16 21:46:51.809552: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2023-11-16 21:46:51.815297: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2023-11-16 21:46:51.818947: I tensorflow/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: my_logs/run_/20231116_214650/plugins/profile/2023_11_16_21_46_51/ONEASH-MacBookAir.local.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1719/Unknown - 16s 9ms/step - loss: 0.5759 - accuracy: 0.8074"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:47:06.881085: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16965855212201369837\n",
      "2023-11-16 21:47:06.881098: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9186912594637338805\n",
      "2023-11-16 21:47:06.881102: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15169961292704684925\n",
      "2023-11-16 21:47:06.881363: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3077956721981424094\n",
      "2023-11-16 21:47:06.881371: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15822333100345431264\n",
      "2023-11-16 21:47:06.881376: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12211016274603612608\n",
      "2023-11-16 21:47:06.881381: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16178481367674624952\n",
      "2023-11-16 21:47:06.881384: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16718629776508865572\n",
      "2023-11-16 21:47:06.881387: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15432785020687462378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 17s 9ms/step - loss: 0.5759 - accuracy: 0.8074 - val_loss: 0.5578 - val_accuracy: 0.8122\n",
      "Epoch 2/5\n",
      "   6/1719 [..............................] - ETA: 17s - loss: 0.4566 - accuracy: 0.8490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:47:07.723606: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17067754536311712732\n",
      "2023-11-16 21:47:07.723620: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4402620364284870993\n",
      "2023-11-16 21:47:07.723623: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6011712177030095339\n",
      "2023-11-16 21:47:07.723627: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12347049224484750268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.5288 - accuracy: 0.8287 - val_loss: 0.5442 - val_accuracy: 0.8292\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.5417 - accuracy: 0.8289 - val_loss: 0.6093 - val_accuracy: 0.8256\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5608 - accuracy: 0.8282 - val_loss: 0.5907 - val_accuracy: 0.8258\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5814 - accuracy: 0.8258 - val_loss: 0.6564 - val_accuracy: 0.8188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x297ca33a0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "logs = Path() / \"my_logs\" / \"run_\" / datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set, callbacks=[tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
