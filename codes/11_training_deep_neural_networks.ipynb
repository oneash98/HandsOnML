{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 심층 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 깔금한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)    \n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 그레이디언트 소실과 폭주 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.1 글로럿과 He 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x147137a30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2 수렴하지 않는 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.activation.leaky_relu.LeakyReLU at 0x28b1a4e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, kernel_initializer='he_normal')\n",
    "keras.layers.LeakyReLU(alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3 배치 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jd/_tlvgz1d22x3k8jpbtplqcsh0000gn/T/ipykernel_69348/3873162892.py:1: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  model.layers[1].updates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.4 그레이디언트 클리핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 사전훈련된 층 재사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1 케라스를 사용한 전이 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.5611 - accuracy: 0.8215 - val_loss: 0.3784 - val_accuracy: 0.8727\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.3452 - accuracy: 0.8844 - val_loss: 0.3185 - val_accuracy: 0.8926\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.3098 - accuracy: 0.8948 - val_loss: 0.2916 - val_accuracy: 0.9008\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2913 - accuracy: 0.9017 - val_loss: 0.2800 - val_accuracy: 0.9076\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2788 - accuracy: 0.9057 - val_loss: 0.2694 - val_accuracy: 0.9108\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2694 - accuracy: 0.9085 - val_loss: 0.2683 - val_accuracy: 0.9053\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2616 - accuracy: 0.9113 - val_loss: 0.2647 - val_accuracy: 0.9101\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2551 - accuracy: 0.9137 - val_loss: 0.2550 - val_accuracy: 0.9168\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2502 - accuracy: 0.9150 - val_loss: 0.2489 - val_accuracy: 0.9163\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2453 - accuracy: 0.9162 - val_loss: 0.2497 - val_accuracy: 0.9198\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2410 - accuracy: 0.9177 - val_loss: 0.2476 - val_accuracy: 0.9165\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2369 - accuracy: 0.9191 - val_loss: 0.2443 - val_accuracy: 0.9145\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2337 - accuracy: 0.9204 - val_loss: 0.2385 - val_accuracy: 0.9165\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2301 - accuracy: 0.9214 - val_loss: 0.2375 - val_accuracy: 0.9195\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2275 - accuracy: 0.9212 - val_loss: 0.2416 - val_accuracy: 0.9178\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2245 - accuracy: 0.9232 - val_loss: 0.2351 - val_accuracy: 0.9210\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2221 - accuracy: 0.9230 - val_loss: 0.2399 - val_accuracy: 0.9178\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2189 - accuracy: 0.9243 - val_loss: 0.2420 - val_accuracy: 0.9148\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2167 - accuracy: 0.9251 - val_loss: 0.2314 - val_accuracy: 0.9185\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2144 - accuracy: 0.9264 - val_loss: 0.2299 - val_accuracy: 0.9208\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.5086 - accuracy: 0.7250 - val_loss: 0.4305 - val_accuracy: 0.7708\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3189 - accuracy: 0.8550 - val_loss: 0.3001 - val_accuracy: 0.8682\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.2244 - accuracy: 0.9400 - val_loss: 0.2298 - val_accuracy: 0.9158\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.1724 - accuracy: 0.9700 - val_loss: 0.1853 - val_accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0927 - accuracy: 0.9850 - val_loss: 0.0702 - val_accuracy: 0.9817\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0411 - accuracy: 0.9900 - val_loss: 0.0542 - val_accuracy: 0.9848\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9858\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 0.9848\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9858\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9868\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9848\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 0.9858\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 0.9868\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 0.9868\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 0.9868\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 0.9878\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 0.9878\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0310 - val_accuracy: 0.9878\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 0.9888\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0300 - val_accuracy: 0.9888\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr = 1e-4)\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 774us/step - loss: 0.0164 - accuracy: 0.9950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01644645445048809, 0.9950000047683716]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2 비지도 사전훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3 보조 작업에서 사전훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 고속 옵테마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1 모멘텀 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2 네스테로프 가속 경사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.4 RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.5 Adam과 Nadam 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1 = 0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.6 학습률 스케줄링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 규제를 사용해 과대적합 피하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1 l1과 l2 규제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation = 'elu', \n",
    "                           kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation = 'elu',\n",
    "                           kernel_initializer = 'he_normal',\n",
    "                           kernel_regularizer = keras.regularizers.l2(0.01))\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2 드롭아웃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.3 몬테 카를로 드롭아웃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.4 맥스-노름 규제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 요약 및 실용적인 가이드라인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 280s 2us/step\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_cifar10_model.h5', save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 9s 5ms/step - loss: 5.1208 - accuracy: 0.1782 - val_loss: 2.1026 - val_accuracy: 0.2398\n",
      "Epoch 2/100\n",
      "  31/1407 [..............................] - ETA: 7s - loss: 2.0473 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 7s 5ms/step - loss: 2.0321 - accuracy: 0.2540 - val_loss: 2.0310 - val_accuracy: 0.2496\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.9244 - accuracy: 0.2935 - val_loss: 1.9731 - val_accuracy: 0.2812\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.8412 - accuracy: 0.3275 - val_loss: 1.9701 - val_accuracy: 0.2926\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7773 - accuracy: 0.3535 - val_loss: 1.8049 - val_accuracy: 0.3376\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7243 - accuracy: 0.3760 - val_loss: 1.7211 - val_accuracy: 0.3786\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6766 - accuracy: 0.3945 - val_loss: 1.7781 - val_accuracy: 0.3462\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6433 - accuracy: 0.4070 - val_loss: 1.6456 - val_accuracy: 0.3970\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6116 - accuracy: 0.4218 - val_loss: 1.6590 - val_accuracy: 0.3972\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5878 - accuracy: 0.4283 - val_loss: 1.6683 - val_accuracy: 0.3980\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5646 - accuracy: 0.4364 - val_loss: 1.6480 - val_accuracy: 0.3992\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5416 - accuracy: 0.4459 - val_loss: 1.6429 - val_accuracy: 0.4106\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5243 - accuracy: 0.4530 - val_loss: 1.5915 - val_accuracy: 0.4200\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5073 - accuracy: 0.4585 - val_loss: 1.5868 - val_accuracy: 0.4276\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4906 - accuracy: 0.4663 - val_loss: 1.5695 - val_accuracy: 0.4384\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4689 - accuracy: 0.4733 - val_loss: 1.5630 - val_accuracy: 0.4332\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4579 - accuracy: 0.4765 - val_loss: 1.5413 - val_accuracy: 0.4458\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4432 - accuracy: 0.4836 - val_loss: 1.5818 - val_accuracy: 0.4388\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4319 - accuracy: 0.4876 - val_loss: 1.5506 - val_accuracy: 0.4402\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4174 - accuracy: 0.4922 - val_loss: 1.5477 - val_accuracy: 0.4446\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4025 - accuracy: 0.4969 - val_loss: 1.5533 - val_accuracy: 0.4480\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3949 - accuracy: 0.4987 - val_loss: 1.5154 - val_accuracy: 0.4542\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3827 - accuracy: 0.5065 - val_loss: 1.5355 - val_accuracy: 0.4478\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3706 - accuracy: 0.5075 - val_loss: 1.5423 - val_accuracy: 0.4454\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3581 - accuracy: 0.5134 - val_loss: 1.5056 - val_accuracy: 0.4646\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3472 - accuracy: 0.5176 - val_loss: 1.5114 - val_accuracy: 0.4550\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3369 - accuracy: 0.5212 - val_loss: 1.5300 - val_accuracy: 0.4536\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3286 - accuracy: 0.5226 - val_loss: 1.5238 - val_accuracy: 0.4522\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3159 - accuracy: 0.5276 - val_loss: 1.5001 - val_accuracy: 0.4744\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3031 - accuracy: 0.5351 - val_loss: 1.5414 - val_accuracy: 0.4592\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3012 - accuracy: 0.5316 - val_loss: 1.5320 - val_accuracy: 0.4588\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2882 - accuracy: 0.5380 - val_loss: 1.4828 - val_accuracy: 0.4794\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.2800 - accuracy: 0.5426 - val_loss: 1.5208 - val_accuracy: 0.4642\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2704 - accuracy: 0.5448 - val_loss: 1.5371 - val_accuracy: 0.4656\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2626 - accuracy: 0.5461 - val_loss: 1.5259 - val_accuracy: 0.4666\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2536 - accuracy: 0.5525 - val_loss: 1.5406 - val_accuracy: 0.4600\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2472 - accuracy: 0.5524 - val_loss: 1.5193 - val_accuracy: 0.4700\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2413 - accuracy: 0.5564 - val_loss: 1.4988 - val_accuracy: 0.4768\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2273 - accuracy: 0.5616 - val_loss: 1.5077 - val_accuracy: 0.4782\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2203 - accuracy: 0.5624 - val_loss: 1.5236 - val_accuracy: 0.4778\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2149 - accuracy: 0.5644 - val_loss: 1.5282 - val_accuracy: 0.4738\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2065 - accuracy: 0.5686 - val_loss: 1.5609 - val_accuracy: 0.4732\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2019 - accuracy: 0.5696 - val_loss: 1.5237 - val_accuracy: 0.4718\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1881 - accuracy: 0.5746 - val_loss: 1.5213 - val_accuracy: 0.4788\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1822 - accuracy: 0.5771 - val_loss: 1.5261 - val_accuracy: 0.4712\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1782 - accuracy: 0.5780 - val_loss: 1.5199 - val_accuracy: 0.4766\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1685 - accuracy: 0.5827 - val_loss: 1.5154 - val_accuracy: 0.4794\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1588 - accuracy: 0.5846 - val_loss: 1.5244 - val_accuracy: 0.4752\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1549 - accuracy: 0.5878 - val_loss: 1.5253 - val_accuracy: 0.4656\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1464 - accuracy: 0.5891 - val_loss: 1.5508 - val_accuracy: 0.4740\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1423 - accuracy: 0.5911 - val_loss: 1.5678 - val_accuracy: 0.4658\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1310 - accuracy: 0.5944 - val_loss: 1.5580 - val_accuracy: 0.4788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x292ae7e50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.4828 - accuracy: 0.4794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.482820987701416, 0.47940000891685486]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('my_cifar10_model.h5')\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 11s 6ms/step - loss: 1.7513 - accuracy: 0.3743 - val_loss: 1.5846 - val_accuracy: 0.4366\n",
      "Epoch 2/100\n",
      "  20/1407 [..............................] - ETA: 7s - loss: 1.5928 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5787 - accuracy: 0.4396 - val_loss: 1.4861 - val_accuracy: 0.4638\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5076 - accuracy: 0.4649 - val_loss: 1.4569 - val_accuracy: 0.4810\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4577 - accuracy: 0.4816 - val_loss: 1.4629 - val_accuracy: 0.4768\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4113 - accuracy: 0.4994 - val_loss: 1.3841 - val_accuracy: 0.5144\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3735 - accuracy: 0.5128 - val_loss: 1.3679 - val_accuracy: 0.5156\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3445 - accuracy: 0.5204 - val_loss: 1.3646 - val_accuracy: 0.5102\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3154 - accuracy: 0.5334 - val_loss: 1.3452 - val_accuracy: 0.5226\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2906 - accuracy: 0.5407 - val_loss: 1.3298 - val_accuracy: 0.5270\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2687 - accuracy: 0.5474 - val_loss: 1.3150 - val_accuracy: 0.5376\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2483 - accuracy: 0.5559 - val_loss: 1.3153 - val_accuracy: 0.5300\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2242 - accuracy: 0.5649 - val_loss: 1.3388 - val_accuracy: 0.5236\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2036 - accuracy: 0.5719 - val_loss: 1.3399 - val_accuracy: 0.5240\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1879 - accuracy: 0.5768 - val_loss: 1.3168 - val_accuracy: 0.5406\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1687 - accuracy: 0.5867 - val_loss: 1.3393 - val_accuracy: 0.5348\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1569 - accuracy: 0.5873 - val_loss: 1.3295 - val_accuracy: 0.5394\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1375 - accuracy: 0.5934 - val_loss: 1.3002 - val_accuracy: 0.5424\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1225 - accuracy: 0.6016 - val_loss: 1.3395 - val_accuracy: 0.5458\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1121 - accuracy: 0.6045 - val_loss: 1.3074 - val_accuracy: 0.5436\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1011 - accuracy: 0.6090 - val_loss: 1.3448 - val_accuracy: 0.5350\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0836 - accuracy: 0.6152 - val_loss: 1.3368 - val_accuracy: 0.5402\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0659 - accuracy: 0.6206 - val_loss: 1.3356 - val_accuracy: 0.5408\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0550 - accuracy: 0.6247 - val_loss: 1.3494 - val_accuracy: 0.5394\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0404 - accuracy: 0.6304 - val_loss: 1.3370 - val_accuracy: 0.5486\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0314 - accuracy: 0.6319 - val_loss: 1.3700 - val_accuracy: 0.5444\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0187 - accuracy: 0.6366 - val_loss: 1.3469 - val_accuracy: 0.5322\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0084 - accuracy: 0.6414 - val_loss: 1.3502 - val_accuracy: 0.5390\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9996 - accuracy: 0.6433 - val_loss: 1.3643 - val_accuracy: 0.5404\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9871 - accuracy: 0.6490 - val_loss: 1.3515 - val_accuracy: 0.5460\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9762 - accuracy: 0.6513 - val_loss: 1.3659 - val_accuracy: 0.5412\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9613 - accuracy: 0.6588 - val_loss: 1.3817 - val_accuracy: 0.5494\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9558 - accuracy: 0.6594 - val_loss: 1.3916 - val_accuracy: 0.5470\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9423 - accuracy: 0.6622 - val_loss: 1.3932 - val_accuracy: 0.5418\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9373 - accuracy: 0.6665 - val_loss: 1.4012 - val_accuracy: 0.5480\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9248 - accuracy: 0.6707 - val_loss: 1.3944 - val_accuracy: 0.5398\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9216 - accuracy: 0.6699 - val_loss: 1.3800 - val_accuracy: 0.5494\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9079 - accuracy: 0.6757 - val_loss: 1.4165 - val_accuracy: 0.5374\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 1.3002 - accuracy: 0.5424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3001855611801147, 0.5424000024795532]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(10):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_cifar10_bn_model.h5', save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model('my_cifar10_bn_model.h5')\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 9s 5ms/step - loss: 1.9244 - accuracy: 0.3135 - val_loss: 1.8595 - val_accuracy: 0.3260\n",
      "Epoch 2/100\n",
      "  31/1407 [..............................] - ETA: 7s - loss: 1.7409 - accuracy: 0.3770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.7110 - accuracy: 0.3948 - val_loss: 1.7837 - val_accuracy: 0.3536\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.6123 - accuracy: 0.4316 - val_loss: 1.6910 - val_accuracy: 0.3970\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.5380 - accuracy: 0.4547 - val_loss: 1.6119 - val_accuracy: 0.4388\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4861 - accuracy: 0.4740 - val_loss: 1.6519 - val_accuracy: 0.4264\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4391 - accuracy: 0.4974 - val_loss: 1.5178 - val_accuracy: 0.4628\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3958 - accuracy: 0.5100 - val_loss: 1.5185 - val_accuracy: 0.4726\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3586 - accuracy: 0.5245 - val_loss: 1.4835 - val_accuracy: 0.4752\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.3235 - accuracy: 0.5410 - val_loss: 1.5382 - val_accuracy: 0.4736\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2925 - accuracy: 0.5491 - val_loss: 1.4843 - val_accuracy: 0.5064\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2630 - accuracy: 0.5620 - val_loss: 1.5033 - val_accuracy: 0.4914\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2318 - accuracy: 0.5749 - val_loss: 1.5196 - val_accuracy: 0.4868\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.2052 - accuracy: 0.5830 - val_loss: 1.4994 - val_accuracy: 0.4992\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1810 - accuracy: 0.5934 - val_loss: 1.4876 - val_accuracy: 0.5082\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1566 - accuracy: 0.6004 - val_loss: 1.5088 - val_accuracy: 0.5028\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1330 - accuracy: 0.6107 - val_loss: 1.5421 - val_accuracy: 0.5074\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1143 - accuracy: 0.6181 - val_loss: 1.4722 - val_accuracy: 0.5148\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0948 - accuracy: 0.6268 - val_loss: 1.5020 - val_accuracy: 0.5166\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0715 - accuracy: 0.6337 - val_loss: 1.5576 - val_accuracy: 0.5014\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0575 - accuracy: 0.6390 - val_loss: 1.5388 - val_accuracy: 0.5034\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0307 - accuracy: 0.6470 - val_loss: 1.5693 - val_accuracy: 0.5052\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0209 - accuracy: 0.6516 - val_loss: 1.5121 - val_accuracy: 0.5012\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1533 - accuracy: 0.6399 - val_loss: 1.6070 - val_accuracy: 0.4482\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.1685 - accuracy: 0.5997 - val_loss: 1.5951 - val_accuracy: 0.4616\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.1229 - accuracy: 0.6167 - val_loss: 1.5030 - val_accuracy: 0.5048\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0227 - accuracy: 0.6491 - val_loss: 1.5306 - val_accuracy: 0.5030\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9749 - accuracy: 0.6672 - val_loss: 1.5322 - val_accuracy: 0.5150\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9459 - accuracy: 0.6779 - val_loss: 1.5768 - val_accuracy: 0.5106\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9385 - accuracy: 0.6798 - val_loss: 1.6042 - val_accuracy: 0.5056\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9366 - accuracy: 0.6819 - val_loss: 1.6225 - val_accuracy: 0.5058\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9199 - accuracy: 0.6898 - val_loss: 1.6125 - val_accuracy: 0.5090\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9137 - accuracy: 0.6891 - val_loss: 1.6522 - val_accuracy: 0.5032\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8966 - accuracy: 0.6966 - val_loss: 1.6022 - val_accuracy: 0.5214\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8834 - accuracy: 0.7015 - val_loss: 1.5991 - val_accuracy: 0.5072\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8789 - accuracy: 0.7028 - val_loss: 1.7211 - val_accuracy: 0.5096\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 5.4283 - accuracy: 0.6406 - val_loss: 1.6051 - val_accuracy: 0.4970\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9494 - accuracy: 0.6756 - val_loss: 1.6103 - val_accuracy: 0.5058\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 1.4722 - accuracy: 0.5148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4722172021865845, 0.5148000121116638]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_cifar10_selu_model.h5', save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis = 0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model('my_cifar10_selu_model.h5')\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 3s 7ms/step - loss: nan - accuracy: 0.1445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.6296823024749756,\n",
       " 3.897990839821952)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n",
      "WARNING:matplotlib.mathtext:Font 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 2.0746 - accuracy: 0.2780 - val_loss: 1.8051 - val_accuracy: 0.3692\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.7811 - accuracy: 0.3728 - val_loss: 1.6579 - val_accuracy: 0.4124\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.6342 - accuracy: 0.4189 - val_loss: 1.7531 - val_accuracy: 0.4034\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.5558 - accuracy: 0.4469 - val_loss: 1.6493 - val_accuracy: 0.4356\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.5008 - accuracy: 0.4678 - val_loss: 1.6266 - val_accuracy: 0.4484\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.4589 - accuracy: 0.4828 - val_loss: 1.6462 - val_accuracy: 0.4366\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.4186 - accuracy: 0.4982 - val_loss: 1.6287 - val_accuracy: 0.4458\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.3499 - accuracy: 0.5228 - val_loss: 1.5127 - val_accuracy: 0.4802\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.2736 - accuracy: 0.5504 - val_loss: 1.5444 - val_accuracy: 0.4774\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.2066 - accuracy: 0.5728 - val_loss: 1.5472 - val_accuracy: 0.4872\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.1376 - accuracy: 0.5948 - val_loss: 1.5316 - val_accuracy: 0.4912\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 1.0676 - accuracy: 0.6201 - val_loss: 1.5203 - val_accuracy: 0.5022\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 0.9960 - accuracy: 0.6435 - val_loss: 1.5545 - val_accuracy: 0.5128\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 0.9318 - accuracy: 0.6667 - val_loss: 1.5719 - val_accuracy: 0.5154\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 2s 7ms/step - loss: 0.8912 - accuracy: 0.6820 - val_loss: 1.6036 - val_accuracy: 0.5152\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
