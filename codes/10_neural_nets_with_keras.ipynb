{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 케라스를 사용한 인공 신경망 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 깔금한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)    \n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 생물학적 뉴런에서 인공 뉴런까지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1 생물학적 뉴런"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2 뉴런을 사용한 논리 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(np.int32)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.4 다층 퍼셉트론과 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.5 회귀를 위한 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.6 분류를 위한 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 케라스로 다층 퍼셉트론 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 텐서플로2 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.24.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/oneash/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (67.3.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/oneash/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.58.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 시퀀셜 API를 사용하여 이미지 분류기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.reshaping.flatten.Flatten at 0x2935b3d90>,\n",
       " <keras.src.layers.core.dense.Dense at 0x292573850>,\n",
       " <keras.src.layers.core.dense.Dense at 0x292573eb0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2925737c0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_4') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07069956,  0.02851167, -0.07412583, ..., -0.04584723,\n",
       "         0.04795287, -0.03125305],\n",
       "       [ 0.03678269, -0.01054567, -0.03338179, ...,  0.05706964,\n",
       "        -0.064698  ,  0.04331894],\n",
       "       [ 0.03103479, -0.02975034, -0.00388635, ..., -0.01207769,\n",
       "        -0.0096683 ,  0.045645  ],\n",
       "       ...,\n",
       "       [ 0.01386178,  0.01302388, -0.07073557, ..., -0.03146289,\n",
       "         0.03631281, -0.05941225],\n",
       "       [ 0.04418054,  0.02033311, -0.01565163, ...,  0.06917742,\n",
       "         0.04748305,  0.04221424],\n",
       "       [-0.01652324,  0.05316241, -0.01560773, ...,  0.04923777,\n",
       "        -0.03417987,  0.06897636]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.7069 - accuracy: 0.7668 - val_loss: 0.5000 - val_accuracy: 0.8358\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 1s 861us/step - loss: 0.4863 - accuracy: 0.8302 - val_loss: 0.4803 - val_accuracy: 0.8274\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 1s 812us/step - loss: 0.4396 - accuracy: 0.8453 - val_loss: 0.4230 - val_accuracy: 0.8548\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 2s 874us/step - loss: 0.4109 - accuracy: 0.8557 - val_loss: 0.4026 - val_accuracy: 0.8618\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 2s 907us/step - loss: 0.3917 - accuracy: 0.8624 - val_loss: 0.3785 - val_accuracy: 0.8672\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 890us/step - loss: 0.3762 - accuracy: 0.8672 - val_loss: 0.3870 - val_accuracy: 0.8636\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3631 - accuracy: 0.8714 - val_loss: 0.3506 - val_accuracy: 0.8798\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3516 - accuracy: 0.8764 - val_loss: 0.3620 - val_accuracy: 0.8738\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3414 - accuracy: 0.8793 - val_loss: 0.3436 - val_accuracy: 0.8790\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 988us/step - loss: 0.3334 - accuracy: 0.8811 - val_loss: 0.3426 - val_accuracy: 0.8758\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 944us/step - loss: 0.3240 - accuracy: 0.8838 - val_loss: 0.3471 - val_accuracy: 0.8800\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3177 - accuracy: 0.8865 - val_loss: 0.3590 - val_accuracy: 0.8692\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 2s 958us/step - loss: 0.3105 - accuracy: 0.8888 - val_loss: 0.3364 - val_accuracy: 0.8800\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 2s 969us/step - loss: 0.3033 - accuracy: 0.8909 - val_loss: 0.3208 - val_accuracy: 0.8870\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 2s 948us/step - loss: 0.2966 - accuracy: 0.8932 - val_loss: 0.3181 - val_accuracy: 0.8862\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2914 - accuracy: 0.8955 - val_loss: 0.3123 - val_accuracy: 0.8870\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 961us/step - loss: 0.2857 - accuracy: 0.8979 - val_loss: 0.3094 - val_accuracy: 0.8860\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 901us/step - loss: 0.2809 - accuracy: 0.8987 - val_loss: 0.3205 - val_accuracy: 0.8866\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2750 - accuracy: 0.9016 - val_loss: 0.3269 - val_accuracy: 0.8820\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2695 - accuracy: 0.9025 - val_loss: 0.3031 - val_accuracy: 0.8888\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 961us/step - loss: 0.2647 - accuracy: 0.9037 - val_loss: 0.2981 - val_accuracy: 0.8914\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 2s 958us/step - loss: 0.2613 - accuracy: 0.9046 - val_loss: 0.3135 - val_accuracy: 0.8864\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 930us/step - loss: 0.2575 - accuracy: 0.9065 - val_loss: 0.2952 - val_accuracy: 0.8946\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 965us/step - loss: 0.2513 - accuracy: 0.9087 - val_loss: 0.2943 - val_accuracy: 0.8936\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2482 - accuracy: 0.9096 - val_loss: 0.3024 - val_accuracy: 0.8880\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2442 - accuracy: 0.9118 - val_loss: 0.3036 - val_accuracy: 0.8920\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2400 - accuracy: 0.9130 - val_loss: 0.2921 - val_accuracy: 0.8922\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 995us/step - loss: 0.2362 - accuracy: 0.9149 - val_loss: 0.2930 - val_accuracy: 0.8924\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 1s 833us/step - loss: 0.2322 - accuracy: 0.9159 - val_loss: 0.2907 - val_accuracy: 0.8956\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 1s 821us/step - loss: 0.2293 - accuracy: 0.9176 - val_loss: 0.3055 - val_accuracy: 0.8894\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 513us/step - loss: 0.3376 - accuracy: 0.8812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.337621808052063, 0.8812000155448914]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.98],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_proba.argmax(axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3 시퀀셜 API를 사용하여 회귀용 다층 퍼셉트론 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 605us/step - loss: 0.8206 - val_loss: 0.5775\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.6710 - val_loss: 0.4500\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 436us/step - loss: 0.4791 - val_loss: 0.4319\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.4615 - val_loss: 0.4206\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 411us/step - loss: 0.4475 - val_loss: 0.4105\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 429us/step - loss: 0.4383 - val_loss: 0.4068\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.4298 - val_loss: 0.4025\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 781us/step - loss: 0.4233 - val_loss: 0.3961\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 582us/step - loss: 0.4159 - val_loss: 0.3889\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 676us/step - loss: 0.4097 - val_loss: 0.3855\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 759us/step - loss: 0.4048 - val_loss: 0.3843\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 424us/step - loss: 0.4002 - val_loss: 0.3787\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 459us/step - loss: 0.3951 - val_loss: 0.3764\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 388us/step - loss: 0.3912 - val_loss: 0.3726\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.3881 - val_loss: 0.3721\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 433us/step - loss: 0.3845 - val_loss: 0.3643\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 444us/step - loss: 0.3893 - val_loss: 0.3650\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3795 - val_loss: 0.3660\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 445us/step - loss: 0.3774 - val_loss: 0.3634\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3742 - val_loss: 0.3568\n",
      "162/162 [==============================] - 0s 252us/step - loss: 0.3852\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3851504921913147"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90019023],\n",
       "       [0.8040142 ],\n",
       "       [2.1368122 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.4 함수형 API를 사용해 복잡한 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "model = keras.Model(inputs = [input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.8069 - val_loss: 0.5012\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 423us/step - loss: 0.5147 - val_loss: 0.4506\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 422us/step - loss: 0.4796 - val_loss: 0.4348\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 423us/step - loss: 0.4602 - val_loss: 0.4250\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 421us/step - loss: 0.4483 - val_loss: 0.4098\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 428us/step - loss: 0.4360 - val_loss: 0.4054\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 503us/step - loss: 0.4280 - val_loss: 0.4071\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 432us/step - loss: 0.4204 - val_loss: 0.3871\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.4199 - val_loss: 0.3823\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 435us/step - loss: 0.4058 - val_loss: 0.3745\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 530us/step - loss: 0.4053 - val_loss: 0.3710\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 421us/step - loss: 0.3927 - val_loss: 0.3615\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 418us/step - loss: 0.3857 - val_loss: 0.3568\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 421us/step - loss: 0.3791 - val_loss: 0.3540\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.3758 - val_loss: 0.3523\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 416us/step - loss: 0.3702 - val_loss: 0.3468\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 415us/step - loss: 0.3648 - val_loss: 0.3439\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 422us/step - loss: 0.3611 - val_loss: 0.3386\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 418us/step - loss: 0.3586 - val_loss: 0.3323\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 424us/step - loss: 0.3569 - val_loss: 0.3375\n",
      "162/162 [==============================] - 0s 297us/step - loss: 0.3656\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 759us/step - loss: 0.9908 - main_output_loss: 0.8911 - aux_output_loss: 1.8884 - val_loss: 0.5852 - val_main_output_loss: 0.5270 - val_aux_output_loss: 1.1089\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 518us/step - loss: 0.5838 - main_output_loss: 0.5340 - aux_output_loss: 1.0322 - val_loss: 0.5223 - val_main_output_loss: 0.4781 - val_aux_output_loss: 0.9206\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 514us/step - loss: 0.5369 - main_output_loss: 0.4990 - aux_output_loss: 0.8780 - val_loss: 0.4949 - val_main_output_loss: 0.4596 - val_aux_output_loss: 0.8134\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 527us/step - loss: 0.5113 - main_output_loss: 0.4810 - aux_output_loss: 0.7840 - val_loss: 0.4726 - val_main_output_loss: 0.4428 - val_aux_output_loss: 0.7405\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 500us/step - loss: 0.4926 - main_output_loss: 0.4665 - aux_output_loss: 0.7273 - val_loss: 0.4626 - val_main_output_loss: 0.4363 - val_aux_output_loss: 0.6987\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 488us/step - loss: 0.4805 - main_output_loss: 0.4573 - aux_output_loss: 0.6891 - val_loss: 0.4465 - val_main_output_loss: 0.4222 - val_aux_output_loss: 0.6659\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.4697 - main_output_loss: 0.4478 - aux_output_loss: 0.6666 - val_loss: 0.4384 - val_main_output_loss: 0.4160 - val_aux_output_loss: 0.6402\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.4585 - main_output_loss: 0.4380 - aux_output_loss: 0.6435 - val_loss: 0.4329 - val_main_output_loss: 0.4108 - val_aux_output_loss: 0.6319\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 491us/step - loss: 0.4490 - main_output_loss: 0.4291 - aux_output_loss: 0.6287 - val_loss: 0.4179 - val_main_output_loss: 0.3971 - val_aux_output_loss: 0.6051\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 507us/step - loss: 0.4522 - main_output_loss: 0.4334 - aux_output_loss: 0.6212 - val_loss: 0.4289 - val_main_output_loss: 0.4101 - val_aux_output_loss: 0.5983\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 633us/step - loss: 0.4334 - main_output_loss: 0.4148 - aux_output_loss: 0.6009 - val_loss: 0.4047 - val_main_output_loss: 0.3851 - val_aux_output_loss: 0.5809\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 531us/step - loss: 0.4268 - main_output_loss: 0.4088 - aux_output_loss: 0.5885 - val_loss: 0.4161 - val_main_output_loss: 0.3977 - val_aux_output_loss: 0.5817\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.4233 - main_output_loss: 0.4058 - aux_output_loss: 0.5804 - val_loss: 0.3892 - val_main_output_loss: 0.3705 - val_aux_output_loss: 0.5578\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.4130 - main_output_loss: 0.3959 - aux_output_loss: 0.5664 - val_loss: 0.3934 - val_main_output_loss: 0.3758 - val_aux_output_loss: 0.5522\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.4083 - main_output_loss: 0.3916 - aux_output_loss: 0.5590 - val_loss: 0.3795 - val_main_output_loss: 0.3616 - val_aux_output_loss: 0.5405\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.4047 - main_output_loss: 0.3886 - aux_output_loss: 0.5500 - val_loss: 0.3743 - val_main_output_loss: 0.3570 - val_aux_output_loss: 0.5295\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.3959 - main_output_loss: 0.3798 - aux_output_loss: 0.5404 - val_loss: 0.3767 - val_main_output_loss: 0.3596 - val_aux_output_loss: 0.5310\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.3886 - main_output_loss: 0.3730 - aux_output_loss: 0.5284 - val_loss: 0.3680 - val_main_output_loss: 0.3518 - val_aux_output_loss: 0.5146\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 491us/step - loss: 0.5674 - main_output_loss: 0.5574 - aux_output_loss: 0.6579 - val_loss: 0.4148 - val_main_output_loss: 0.3924 - val_aux_output_loss: 0.6168\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.4186 - main_output_loss: 0.4015 - aux_output_loss: 0.5726 - val_loss: 0.3958 - val_main_output_loss: 0.3791 - val_aux_output_loss: 0.5458\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 364us/step - loss: 0.4585 - main_output_loss: 0.4432 - aux_output_loss: 0.5959\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.5 서브클래싱 API로 동적 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.6 모델 저장과 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 560us/step - loss: 1.6473 - val_loss: 0.9025\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 414us/step - loss: 0.8752 - val_loss: 0.7491\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.7654 - val_loss: 0.6858\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 439us/step - loss: 0.7076 - val_loss: 0.6431\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 534us/step - loss: 0.6658 - val_loss: 0.6086\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 411us/step - loss: 0.6325 - val_loss: 0.5814\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.6056 - val_loss: 0.5587\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.5840 - val_loss: 0.5401\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 431us/step - loss: 0.5655 - val_loss: 0.5246\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 423us/step - loss: 0.5501 - val_loss: 0.5123\n",
      "162/162 [==============================] - 0s 279us/step - loss: 0.5511\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.7 콜백 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 336us/step - loss: 0.5374\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 314us/step - loss: 0.5268\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 321us/step - loss: 0.5173\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 331us/step - loss: 0.5094\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 324us/step - loss: 0.5021\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 312us/step - loss: 0.4959\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 313us/step - loss: 0.4902\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 315us/step - loss: 0.4851\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 389us/step - loss: 0.4808\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 319us/step - loss: 0.4766\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.4728 - val_loss: 0.4449\n",
      "Epoch 2/10\n",
      "309/363 [========================>.....] - ETA: 0s - loss: 0.4714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 787us/step - loss: 0.4692 - val_loss: 0.4418\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 428us/step - loss: 0.4658 - val_loss: 0.4389\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 436us/step - loss: 0.4628 - val_loss: 0.4370\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 419us/step - loss: 0.4601 - val_loss: 0.4348\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 417us/step - loss: 0.4576 - val_loss: 0.4321\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 419us/step - loss: 0.4551 - val_loss: 0.4307\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 428us/step - loss: 0.4527 - val_loss: 0.4294\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 420us/step - loss: 0.4507 - val_loss: 0.4267\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 460us/step - loss: 0.4486 - val_loss: 0.4253\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.4466 - val_loss: 0.4234\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.4448 - val_loss: 0.4227\n",
      "Epoch 3/100\n",
      "  1/363 [..............................] - ETA: 0s - loss: 0.3646"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 528us/step - loss: 0.4427 - val_loss: 0.4204\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 421us/step - loss: 0.4411 - val_loss: 0.4187\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.4393 - val_loss: 0.4198\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 412us/step - loss: 0.4377 - val_loss: 0.4167\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 440us/step - loss: 0.4361 - val_loss: 0.4155\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 435us/step - loss: 0.4346 - val_loss: 0.4131\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 437us/step - loss: 0.4328 - val_loss: 0.4120\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 418us/step - loss: 0.4313 - val_loss: 0.4106\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 417us/step - loss: 0.4296 - val_loss: 0.4092\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.4282 - val_loss: 0.4093\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 425us/step - loss: 0.4270 - val_loss: 0.4071\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 426us/step - loss: 0.4255 - val_loss: 0.4067\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 526us/step - loss: 0.4239 - val_loss: 0.4044\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 437us/step - loss: 0.4229 - val_loss: 0.4034\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 454us/step - loss: 0.4213 - val_loss: 0.4023\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 418us/step - loss: 0.4196 - val_loss: 0.4006\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 419us/step - loss: 0.4185 - val_loss: 0.3993\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 423us/step - loss: 0.4171 - val_loss: 0.3983\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 410us/step - loss: 0.4156 - val_loss: 0.3974\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 411us/step - loss: 0.4142 - val_loss: 0.3964\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 415us/step - loss: 0.4132 - val_loss: 0.3948\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 418us/step - loss: 0.4119 - val_loss: 0.3949\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.4105 - val_loss: 0.3941\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.4096 - val_loss: 0.3920\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.4082 - val_loss: 0.3911\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.4071 - val_loss: 0.3902\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.4061 - val_loss: 0.3892\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 407us/step - loss: 0.4053 - val_loss: 0.3887\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.4043 - val_loss: 0.3880\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.4032 - val_loss: 0.3869\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 417us/step - loss: 0.4021 - val_loss: 0.3865\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 416us/step - loss: 0.4012 - val_loss: 0.3853\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 416us/step - loss: 0.4004 - val_loss: 0.3850\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 413us/step - loss: 0.3996 - val_loss: 0.3838\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3983 - val_loss: 0.3843\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.3979 - val_loss: 0.3825\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 427us/step - loss: 0.3968 - val_loss: 0.3814\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 425us/step - loss: 0.3960 - val_loss: 0.3813\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.3950 - val_loss: 0.3807\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 417us/step - loss: 0.3942 - val_loss: 0.3798\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 417us/step - loss: 0.3934 - val_loss: 0.3786\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 414us/step - loss: 0.3925 - val_loss: 0.3782\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 411us/step - loss: 0.3915 - val_loss: 0.3778\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 416us/step - loss: 0.3909 - val_loss: 0.3766\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3898 - val_loss: 0.3759\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 409us/step - loss: 0.3893 - val_loss: 0.3751\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3883 - val_loss: 0.3747\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.3873 - val_loss: 0.3748\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3865 - val_loss: 0.3731\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3859 - val_loss: 0.3725\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.3850 - val_loss: 0.3721\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.3843 - val_loss: 0.3715\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3835 - val_loss: 0.3705\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3825 - val_loss: 0.3706\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3819 - val_loss: 0.3696\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 416us/step - loss: 0.3812 - val_loss: 0.3684\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3806 - val_loss: 0.3678\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3797 - val_loss: 0.3672\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.3789 - val_loss: 0.3665\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3781 - val_loss: 0.3661\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3774 - val_loss: 0.3660\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3767 - val_loss: 0.3644\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.3760 - val_loss: 0.3645\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.3754 - val_loss: 0.3640\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3743 - val_loss: 0.3630\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.3737 - val_loss: 0.3622\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3730 - val_loss: 0.3617\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3723 - val_loss: 0.3609\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3718 - val_loss: 0.3605\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3706 - val_loss: 0.3607\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 407us/step - loss: 0.3702 - val_loss: 0.3598\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 410us/step - loss: 0.3693 - val_loss: 0.3587\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3685 - val_loss: 0.3577\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3679 - val_loss: 0.3576\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 468us/step - loss: 0.3670 - val_loss: 0.3568\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.3665 - val_loss: 0.3560\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3658 - val_loss: 0.3564\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3652 - val_loss: 0.3555\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3643 - val_loss: 0.3555\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3638 - val_loss: 0.3540\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3629 - val_loss: 0.3532\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3618 - val_loss: 0.3547\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3617 - val_loss: 0.3524\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3612 - val_loss: 0.3514\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3603 - val_loss: 0.3514\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3598 - val_loss: 0.3510\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3591 - val_loss: 0.3504\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.3583 - val_loss: 0.3497\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 404us/step - loss: 0.3579 - val_loss: 0.3493\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3574 - val_loss: 0.3489\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3567 - val_loss: 0.3480\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3561 - val_loss: 0.3486\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 402us/step - loss: 0.3555 - val_loss: 0.3472\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 407us/step - loss: 0.3552 - val_loss: 0.3467\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3545 - val_loss: 0.3465\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.3540 - val_loss: 0.3465\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3534 - val_loss: 0.3455\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 407us/step - loss: 0.3528 - val_loss: 0.3451\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print('\\nval/train: {}:.2f'.format(logs['val_loss'] / logs['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.8 텐서보드를 사용해 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 0s 854us/step - loss: 2.0421 - val_loss: 0.8976\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.8169 - val_loss: 0.7202\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 443us/step - loss: 0.7154 - val_loss: 0.6543\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 441us/step - loss: 0.6616 - val_loss: 0.6105\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 473us/step - loss: 0.6253 - val_loss: 0.5797\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 433us/step - loss: 0.5967 - val_loss: 0.5545\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 435us/step - loss: 0.5749 - val_loss: 0.5344\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 450us/step - loss: 0.5570 - val_loss: 0.5197\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 432us/step - loss: 0.5424 - val_loss: 0.5064\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 432us/step - loss: 0.5302 - val_loss: 0.4948\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.5200 - val_loss: 0.4857\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 450us/step - loss: 0.5113 - val_loss: 0.4770\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 440us/step - loss: 0.5044 - val_loss: 0.4700\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 457us/step - loss: 0.4973 - val_loss: 0.4642\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 430us/step - loss: 0.4915 - val_loss: 0.4578\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 458us/step - loss: 0.4860 - val_loss: 0.4540\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.4810 - val_loss: 0.4485\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 429us/step - loss: 0.4772 - val_loss: 0.4443\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.4726 - val_loss: 0.4412\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 437us/step - loss: 0.4693 - val_loss: 0.4376\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 439us/step - loss: 0.4653 - val_loss: 0.4344\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 435us/step - loss: 0.4623 - val_loss: 0.4318\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 437us/step - loss: 0.4597 - val_loss: 0.4296\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.4572 - val_loss: 0.4283\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 428us/step - loss: 0.4546 - val_loss: 0.4252\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 427us/step - loss: 0.4515 - val_loss: 0.4244\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 526us/step - loss: 0.4500 - val_loss: 0.4215\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 436us/step - loss: 0.4476 - val_loss: 0.4201\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.4457 - val_loss: 0.4187\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 438us/step - loss: 0.4438 - val_loss: 0.4173\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 신경망 하이퍼파라미터 튜닝하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate = 3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "keras_reg = KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.8000 - val_loss: 0.5068\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.5069 - val_loss: 0.4609\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.4738 - val_loss: 0.4332\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 407us/step - loss: 0.4541 - val_loss: 0.4264\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.4442 - val_loss: 0.4158\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.4346 - val_loss: 0.4121\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 401us/step - loss: 0.4297 - val_loss: 0.4040\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.4219 - val_loss: 0.3968\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 401us/step - loss: 0.4154 - val_loss: 0.3921\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 401us/step - loss: 0.4105 - val_loss: 0.3910\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.4049 - val_loss: 0.3867\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3991 - val_loss: 0.3790\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 395us/step - loss: 0.3940 - val_loss: 0.3781\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3895 - val_loss: 0.3764\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3867 - val_loss: 0.3672\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3814 - val_loss: 0.3721\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3856 - val_loss: 0.3619\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 395us/step - loss: 0.3760 - val_loss: 0.3658\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3744 - val_loss: 0.3592\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 465us/step - loss: 0.3716 - val_loss: 0.3543\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3682 - val_loss: 0.3631\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3724 - val_loss: 0.3633\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3646 - val_loss: 0.3625\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 403us/step - loss: 0.3627 - val_loss: 0.3528\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 413us/step - loss: 0.3605 - val_loss: 0.3649\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 411us/step - loss: 0.3614 - val_loss: 0.3471\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3591 - val_loss: 0.3495\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 401us/step - loss: 0.3614 - val_loss: 0.3440\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 406us/step - loss: 0.3568 - val_loss: 0.3423\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 409us/step - loss: 0.3539 - val_loss: 0.3412\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 421us/step - loss: 0.3618 - val_loss: 0.3425\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 416us/step - loss: 0.3505 - val_loss: 0.3406\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3484 - val_loss: 0.3460\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 593us/step - loss: 0.3481 - val_loss: 0.3432\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 428us/step - loss: 0.3494 - val_loss: 0.3422\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 416us/step - loss: 0.3500 - val_loss: 0.3483\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 414us/step - loss: 0.3457 - val_loss: 0.3367\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 418us/step - loss: 0.3445 - val_loss: 0.3345\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 409us/step - loss: 0.3628 - val_loss: 0.3637\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.3464 - val_loss: 0.3363\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 505us/step - loss: 0.3423 - val_loss: 0.3471\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 407us/step - loss: 0.3493 - val_loss: 0.3297\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3427 - val_loss: 0.3312\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3440 - val_loss: 0.3323\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 539us/step - loss: 0.3379 - val_loss: 0.3337\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3364 - val_loss: 0.3289\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 472us/step - loss: 0.3517 - val_loss: 0.3412\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3439 - val_loss: 0.3304\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 395us/step - loss: 0.3382 - val_loss: 0.3299\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3371 - val_loss: 0.3295\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3355 - val_loss: 0.3280\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.3344 - val_loss: 0.3276\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3345 - val_loss: 0.3259\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3415 - val_loss: 0.6150\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3414 - val_loss: 0.3275\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 395us/step - loss: 0.3335 - val_loss: 0.3335\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3364 - val_loss: 0.3288\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3400 - val_loss: 0.3242\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 401us/step - loss: 0.3335 - val_loss: 0.3255\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 409us/step - loss: 0.3328 - val_loss: 0.3237\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 465us/step - loss: 0.3406 - val_loss: 0.4381\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 400us/step - loss: 0.3332 - val_loss: 0.3258\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.3317 - val_loss: 0.3213\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3308 - val_loss: 0.3250\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.3356 - val_loss: 0.3206\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3287 - val_loss: 0.3262\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3395 - val_loss: 0.3188\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 405us/step - loss: 0.3309 - val_loss: 0.3213\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.3325 - val_loss: 0.3220\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 409us/step - loss: 0.3323 - val_loss: 0.3205\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.4785 - val_loss: 0.3321\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.3453 - val_loss: 0.3249\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 393us/step - loss: 0.3429 - val_loss: 0.3221\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.3318 - val_loss: 0.3283\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3378 - val_loss: 0.3246\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3533 - val_loss: 0.3233\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3322 - val_loss: 0.3224\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3288 - val_loss: 0.3194\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.3292 - val_loss: 0.3285\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.3294 - val_loss: 0.3173\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3292 - val_loss: 0.3182\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 393us/step - loss: 0.3264 - val_loss: 0.3172\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 395us/step - loss: 0.3277 - val_loss: 0.3198\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 392us/step - loss: 0.3272 - val_loss: 0.3241\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 399us/step - loss: 0.3301 - val_loss: 0.3176\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3266 - val_loss: 0.3167\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.3316 - val_loss: 0.3302\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3302 - val_loss: 0.3198\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3253 - val_loss: 0.3166\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3247 - val_loss: 0.3198\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 408us/step - loss: 0.3263 - val_loss: 0.3210\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 391us/step - loss: 0.3262 - val_loss: 0.3145\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3244 - val_loss: 0.3276\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 392us/step - loss: 0.3248 - val_loss: 0.3135\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 394us/step - loss: 0.3224 - val_loss: 0.3218\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 396us/step - loss: 0.3248 - val_loss: 0.3197\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 398us/step - loss: 0.3247 - val_loss: 0.3172\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3334 - val_loss: 0.3217\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3276 - val_loss: 0.3240\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 397us/step - loss: 0.3291 - val_loss: 0.3211\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x113422cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x113422cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 451us/step\n",
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data = (X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter n_neurons for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(n_neurons=79)`\nCheck the list of available parameters with `estimator.get_params().keys()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb Cell 78\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m param_distribs \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn_hidden\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn_neurons\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: reciprocal(\u001b[39m3e-4\u001b[39m, \u001b[39m3e-2\u001b[39m)\u001b[39m.\u001b[39mrvs(\u001b[39m1000\u001b[39m)\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m rnd_search_cv \u001b[39m=\u001b[39m RandomizedSearchCV(keras_reg, param_distribs, n_iter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m rnd_search_cv\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                   validation_data\u001b[39m=\u001b[39;49m(X_valid, y_valid),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y160sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                   callbacks\u001b[39m=\u001b[39;49m[keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:674\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m parameters\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    672\u001b[0m         cloned_parameters[k] \u001b[39m=\u001b[39m clone(v, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 674\u001b[0m     estimator \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcloned_parameters)\n\u001b[1;32m    676\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    678\u001b[0m X_train, y_train \u001b[39m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scikeras/wrappers.py:1165\u001b[0m, in \u001b[0;36mBaseWrapper.set_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{param: value})\n\u001b[1;32m   1162\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   1163\u001b[0m             \u001b[39m# Give a SciKeras specific user message to aid\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m             \u001b[39m# in moving from the Keras wrappers\u001b[39;00m\n\u001b[0;32m-> 1165\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1166\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m for estimator \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1167\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mThis issue can likely be resolved by setting this parameter\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1168\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m in the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m constructor:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1169\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1170\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheck the list of available parameters with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1171\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m `estimator.get_params().keys()`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1172\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1173\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter n_neurons for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(n_neurons=79)`\nCheck the list of available parameters with `estimator.get_params().keys()`"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb Cell 79\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y200sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rnd_search_cv\u001b[39m.\u001b[39;49mbest_params_\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb Cell 80\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneash/ONEASH/Coding/HandsOnML2/codes/10_neural_nets_with_keras.ipynb#Y201sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rnd_search_cv\u001b[39m.\u001b[39;49mbest_score_\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 은닉층 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 은닉층의 뉴런 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3 학습률, 배치 크기 그리고 다른 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
